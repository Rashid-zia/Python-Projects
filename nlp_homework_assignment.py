# -*- coding: utf-8 -*-
"""NLP_homework_assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pGDopxX_pqATG38tz-qYpiUo4QeUwkOK
"""

# Commented out IPython magic to ensure Python compatibility.
# ── Colab Cell: paste and run ─────────────────────────────────────────────────

# 1) Inline plots & install deps
# %matplotlib inline
!pip install -q requests beautifulsoup4 textblob nltk wordcloud

# 2) Imports
import requests, re
from bs4 import BeautifulSoup
from textblob import TextBlob
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter

# 3) NLTK setup (only stopwords & wordnet/tagger for TextBlob)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)

# 4) Scraper
def fetch_reviews(url):
    headers = {'User-Agent':'Mozilla/5.0'}
    try:
        r = requests.get(url, headers=headers); r.raise_for_status()
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return []
    soup = BeautifulSoup(r.text, 'html.parser')
    # Metacritic‐style reviews
    reviews = [div.get_text(strip=True)
               for div in soup.select('div.text.show-more__control')]
    if reviews: return reviews
    # fallback: long <p> blocks
    paras = [p.get_text(strip=True) for p in soup.find_all('p')
             if len(p.get_text(strip=True))>50]
    if paras: return paras
    # last resort: body‐chunks
    return [ln.strip() for ln in soup.body.get_text().split('\n') if len(ln.strip())>50]

# 5) Sentiment
def analyze_sentiments(reviews):
    sumry={'positive':0,'neutral':0,'negative':0}; labels=[]
    for rev in reviews:
        p = TextBlob(rev).sentiment.polarity
        if   p>0: sumry['positive']+=1; labels.append('Positive')
        elif p<0: sumry['negative']+=1; labels.append('Negative')
        else:     sumry['neutral'] +=1; labels.append('Neutral')
    return sumry, labels

# 6) Word lists (regex tokenizer)
def extract_frequent_words(reviews, top_n=30):
    sw = set(stopwords.words('english'))
    toks = re.findall(r'\b\w+\b', ' '.join(reviews).lower())
    filt = [w for w in toks if w not in sw and len(w)>2]
    return Counter(filt).most_common(top_n)

def extract_longest_words(reviews, top_n=30):
    toks = re.findall(r'\b\w+\b', ' '.join(reviews).lower())
    uniq = {w for w in toks if len(w)>3}
    sorted_by_len = sorted(uniq, key=len, reverse=True)
    return [(w,len(w)) for w in sorted_by_len[:top_n]]

# 7) Word-cloud helper
def show_wordcloud(data, title):
    if not data:
        print(f"No data to generate word-cloud: {title}")
        return
    if isinstance(data[0], tuple):
        freqs = dict(data)
        wc = WordCloud(width=800, height=400, background_color='white')\
             .generate_from_frequencies(freqs)
    else:
        wc = WordCloud(width=800, height=400, background_color='white')\
             .generate(' '.join(data))
    plt.figure(figsize=(10,5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.show()

# ── Main: three examples ──────────────────────────────────────────────────────
if __name__ == "__main__":
    urls = [
      "https://www.imdb.com/title/tt0111161/reviews",           # IMDb
      "https://www.metacritic.com/movie/inception/user-reviews",
      "https://www.metacritic.com/game/pc/cyberpunk-2077/user-reviews"
    ]

    for url in urls:
        print("\n" + "="*80)
        print("URL:", url)

        # 1) Fetch & count
        reviews = fetch_reviews(url)
        print("\n# Reviews:", len(reviews))

        # 2) Sentiment analysis
        sent_sum, sent_labels = analyze_sentiments(reviews)
        print("\nSentiment Analysis Results:", sent_sum)

        # 3) Word-cloud of sentiments
        print("\nSentiment Word Cloud:")
        show_wordcloud(sent_labels, "Sentiment")

        # 4) Top 30 most used words
        freq30 = extract_frequent_words(reviews, 30)
        print("\n30 Most Used Words:")
        for w,c in freq30: print(f"  {w}: {c}")

        # 5) Word-cloud of most used words
        print("\nWord Cloud of 30 Most Used Words:")
        show_wordcloud(freq30, "Top 30 Frequent Words")

        # 6) Top 30 longest words
        long30 = extract_longest_words(reviews, 30)
        print("\n30 Longest Words:")
        for w,l in long30: print(f"  {w}: {l}")

        # 7) Word-cloud of longest words
        print("\nWord Cloud of 30 Longest Words:")
        show_wordcloud(long30, "Top 30 Longest Words")







